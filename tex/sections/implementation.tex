\section{Spatial augmented reality}\label{sec:sar}

Projection mapping is a \gls{sar} \cite{Bimber2005} approach in which video projectors are used to overlay virtual geometry on top of real objects or surfaces, allowing the creation of an immersive environment that together with 3D perception systems can be used to develop interactive interfaces that show contextual information for helping or teaching human operators performing complex tasks faster. The next sections describe the mathematical modeling of video projectors within the \gls{opengl} and the associated calibration that is necessary for performing proper 3D rendering of the virtual world in order to achieve high accuracy projection.


\subsection{Projector modeling}\label{sec:projector-modeling}

Over the years, several projection technologies were developed according to the requirements of color fidelity / saturation, image sharpness, brightness, contrast, refresh rate and price. Currently, the video projection market is split between reflective \gls{dlp} and transmissive \gls{lcd} projection technology, with a small percentage consisting of a hybrid between the two technologies, such as the \gls{lcos} projectors. For video projection mapping purposes, reflective projectors are usually better suited than the remaining technologies given their ability to create images with smaller gaps between the projected pixels (smoother images) and they also have higher contrast, better color accuracy / uniformity, fewer dead pixels and the image quality does not degrade over time.

Despite the wide range of technologies and hardware configurations (example for \gls{dlp} shown in \cref{fig:dlp-projector-diagram-dmd}), the output of a video projector can be seen as an inverse pinhole camera (diagram shown in \cref{fig:camera-intrinsics-2}) due to the grid disposition of the projected image and the very low distortion that modern projectors have. As such, rendering of 3D virtual environments for spatial augmented reality can be performed efficiently using an extended version of the \gls{opengl} projection matrix\footnote{\url{http://ksimek.github.io/2013/06/03/calibrated\_cameras\_in\_opengl}} (presented in \cref{eq:projection-matrix}).

\vspace{-0.5em}
\begin{figure}[H]
	\centering
	\includegraphics[height=.135\textheight]{dlp-projector-diagram-dmd}
	\caption[Diagram of a single chip \glsentrytext{dlp} projector]{Diagram of a single chip \glsentrytext{dlp} projector\protect\footnotemark}
	\label{fig:dlp-projector-diagram-dmd}
	\vspace{-0.7em}
\end{figure}
\footnotetext[\numexpr\value{footnote}]{\url{https://vimeo.com/blog/post/display-tech-home-projectors}}
\vspace{-0.8em}

The extension of the default \gls{opengl} projection matrix is of critical importance for being able to model video projectors as \gls{opengl} cameras because unlike normal cameras that have a centered lens (which results in a principal point close to the image center), projectors normally have an off-centered lens (causing the principal point to be around the bottom of the image). This is due to the fact that projector lenses are assembled slightly tilted for redirecting the projection light upwards. This way the projector can be installed almost parallel to a table / ceiling and its projected image will not intersect the table / ceiling. In higher-end projectors, the lens shift\footnote{\url{https://www.projectorpeople.com/resources/lens_shift.asp}} can even be adjusted after the projector is manufactured, making its installation more flexible (diagram in \cref{fig:lens-shift--keystone}). The lower cost alternative to lens shift is keystone correction, but this approach is typically avoided in \gls{sar} systems since it distorts the image and wastes projection pixels.
%As such, \gls{sar} systems must take great care when generating the image that will be projected, otherwise the overlap error between the physical and virtual objects will be too much for making the system usable.

\vspace{-0.5em}
\begin{figure}[H]
	\centering
	\includegraphics[width=.77\textwidth]{lens-shift--keystone}
	\caption[Diagram of lens shift (left) and keystone correction (right)]{Diagram of lens shift (left) and keystone correction (right)\protect\footnotemark}
	\label{fig:lens-shift--keystone}
	\vspace{-0.7em}
\end{figure}
\footnotetext[\numexpr\value{footnote}]{\url{http://www.theprojectorexpert.com/projector-lens-shift}}
\vspace{-1em}

The perspective projection of vertex data is a pivotal stage in any rendering engine. In \gls{opengl}, the projection matrix is one of the 4 main vertex transform stages (presented in \cref{fig:opengl-mvp}) and is responsible for converting 3D geometry from the camera space into the clipping space (as depicted in \cref{fig:gl-projection-matrix}). This matrix can be created using the $glOrtho$ function (matrix shown in \cref{eq:glOrtho}) for orthographic projection or the $glFrustum$ function (matrix presented in \cref{eq:glFrustum}) for perspective projection. Looking at both matrices it can be seen that the $glFrustum$ can be decomposed to include the $glOrtho$ matrix and a $Q$ matrix (shown in \cref{eq:Q}) that corresponds to the intrinsic parameters of a pinhole camera with the same focal length for the $X$ and $Y$ axis along with the principal point at the origin of the camera coordinate system while having no axis skew. The main difference between the $Q$ matrix and the traditional $K$ matrix \cite{Hartley2003} (shown in \cref{eq:k}) is the bottom row of $Q$, in which the $w$ component of the homogeneous matrix was set to the $-Z$ coordinate. This is a matrix optimization associated with the perspective z-divide that is required to convert 4D homogeneous coordinates to 3D Cartesian coordinates. It has a negative sign because by default the \gls{opengl} camera is setup to look along the $-Z$ axis (as can be seen in \cref{fig:gl-projection-matrix}), and to simplify and optimize the calculations using the similar triangles properties (displayed in \cref{fig:similar-triangles}), the $Z$ coordinate needs to be negated for having a positive value. Looking at \cref{eq:k2}, it can be seen that $K$ can be adapted to take this sign inversion into consideration, allowing to map the $K'$ values into the $Q$ matrix.

\vspace{-0.3em}
\begin{figure}[H]
	\centering
	\includegraphics[width=.99\textwidth]{opengl-mvp}
	\caption[OpenGL rendering pipeline]{OpenGL coordinates transform pipeline\protect\footnotemark}
	\label{fig:opengl-mvp}
	\vspace{-0.7em}
\end{figure}
\footnotetext[\numexpr\value{footnote}]{\url{https://www.ntu.edu.sg/home/ehchua/programming/opengl/CG_BasicsTheory.html}}
\vspace{-1em}

On the other hand, for being able to bridge between the intrinsic parameters estimated by the OpenCV $calib3d$ module ($K$) and the underlying \gls{opengl} implementation, the relative orientation of their respective cameras must also be taken into account. Namely, switching from the reference system in \cref{fig:camera-intrinsics-2} to the one on left side of \cref{fig:gl-projection-matrix} requires the adjustment of the principal point in the $Y$ axis (because in OpenGL the $Y+$ goes up and in OpenCV the $Y+$ goes down, which results in $Cy'=ImageHeight-Cy$). The other intrinsic parameters are not affected and can be mapped directly into the extended $Q$ matrix shown in \cref{eq:q2}, allowing the usage of the \gls{opengl} rendering pipeline to model pinhole cameras with different focal lengths for the $X$ and $Y$ axis ($Fx$ and $Fy$) while also supporting arbitrary axis skew ($S$) and non-centered principal point ($Cx$ and $Cy$). On this particular use case, the \gls{opengl} frustum clipping planes were defined as $Near=0.1\ meters, Far=5.0\ meters$, and since $Left=0\ pixels, Bottom=0\ pixels$, the $glOrtho$ matrix can be simplified (as seen in \cref{eq:glOrtho2}) for optimizing the creation of the extended \gls{opengl} perspective projection matrix that takes into consideration the full camera intrinsic parameters shown in \cref{eq:k}.

The correction of lens distortion typically uses 3 coefficients for removing radial distortions and 2 coefficients for accounting for the tangential distortions. Despite being relatively small in \gls{dlp} projectors, they can be removed using an \gls{opengl} vertex shader or in a post-processing stage after 3D rendering using the OpenCV calib3d module.

\begin{figure}[H]
	\begin{floatrow}[2]
		\ffigbox[\FBwidth]
		{\includegraphics[height=.13\textheight]{camera-intrinsics-2}}
		{\caption[Pinhole camera model used in the OpenCV calib3d module]{Pinhole camera model used in OpenCV calib3d module\footnotemark}\label{fig:camera-intrinsics-2}\vspace{-0.5em}}
		\hspace{0.03em}
		\ffigbox[\FBwidth]
		{\includegraphics[height=.104\textheight]{gl-projection-matrix}}
		{\caption[{\glsentrytext{opengl} perspective frustum (left) with the associated \glsentrytext{ndc} cube}]{{\glsentrytext{opengl} perspective frustum (left) with the associated \glsentrytext{ndc} cube}\protect\footnotemark}\label{fig:gl-projection-matrix}\vspace{-0.5em}}
	\end{floatrow}
\end{figure}

\vspace{-0.7em}
\begin{figure}[H]
	\centering
	\includegraphics[width=.85\textwidth]{similar-triangles}
	\caption[Diagram showing the similar triangles properties]{Diagram showing the similar triangles properties\protect\footnotemark}
	\label{fig:similar-triangles}
	\vspace{-0.5em}
\end{figure}

\vspace{-0.7em}
\footnotetext[\numexpr\value{footnote}-2]{\url{https://docs.opencv.org/master/d9/d0c/group__calib3d.html}}
\footnotetext[\numexpr\value{footnote}-1]{\url{http://www.songho.ca/opengl/gl_projectionmatrix.html}}
\footnotetext[\value{footnote}]{\url{http://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix}}


{
	\setlength\arraycolsep{.15em}
	\tiny

	\begin{equation}\label{eq:glOrtho}
		glOrtho{=}
		\begin{bmatrix}
			\vfrac{2}{Right - Left} & 0 & 0 & -\vfrac{Right + Left}{Right - Left} \\[\smallskipamount]
			0 & \vfrac{2}{Top - Bottom} & 0 & -\vfrac{Top + Bottom}{Top - Bottom} \\[\smallskipamount]
			0 & 0 & -\vfrac{2}{Far - Near} & -\vfrac{Far + Near}{Far - Near} \\[\smallskipamount]
			0 & 0 & 0 & 1
		\end{bmatrix}
	\end{equation}

	\begin{equation}\label{eq:Q}
		Q{=}
		\begin{bmatrix}
			Near & 0 & 0 & 0 \\[\smallskipamount]
			0 & Near & 0 & 0 \\[\smallskipamount]
			0 & 0 & Near + Far & Near \times Far \\[\smallskipamount]
			0 & 0 & -1 & 0
		\end{bmatrix}
	\end{equation}

	\begin{equation}\label{eq:glFrustum}
		\begin{split}
			glFrustum = PerspectiveProjectionMatrix = glOrtho \times Q
			\\=
			\begin{bmatrix}
				\vfrac{2 \times Near}{Right - Left} & 0 & \vfrac{Right + Left}{Right - Left} & 0 \\[\smallskipamount]
				0 & \vfrac{2 \times Near}{Top - Bottom} & \vfrac{Top + Bottom}{Top - Bottom} & 0 \\[\smallskipamount]
				0 & 0 & -\vfrac{Far + Near}{Far - Near} & -\vfrac{2 \times Far \times Near}{Far - Near} \\[\smallskipamount]
				0 & 0 & -1 & 0
			\end{bmatrix}
		\end{split}
	\end{equation}

	\begin{equation}\label{eq:k}
		K{=}
		\begin{bmatrix}
			Fx & S & Cx & 0 \\[\smallskipamount]
			0 & Fy & Cy & 0 \\[\smallskipamount]
			0 & 0 & 1 & 0
		\end{bmatrix}
	\end{equation}
	
	\begin{equation}\label{eq:k2}
		K'{=}
		K
		\times
		\begin{bmatrix}
		1 & 0 & 0 & 0 \\[\smallskipamount]
		0 & 1 & 0 & 0 \\[\smallskipamount]
		0 & 0 & -1 & 0 \\[\smallskipamount]
		0 & 0 & 0 & 1
		\end{bmatrix}
		=
		\begin{bmatrix}
		Fx & S & -Cx & 0 \\[\smallskipamount]
		0 & Fy & -Cy & 0 \\[\smallskipamount]
		0 & 0 & -1 & 0
		\end{bmatrix}
	\end{equation}

	\begin{equation}\label{eq:q2}
		Q'{=}
		\begin{bmatrix}
			Fx & S & -Cx & 0 \\[\smallskipamount]
			0 & Fy & -(IHeight - Cy) & 0 \\[\smallskipamount]
			0 & 0 & Near + Far & Near \times Far \\[\smallskipamount]
			0 & 0 & -1 & 0
		\end{bmatrix}
	\end{equation}

	\begin{equation}\label{eq:glOrtho2}
		glOrtho'{=}
		\begin{bmatrix}
			\vfrac{2}{IWidth} & 0 & 0 & -1 \\[\smallskipamount]
			0 & \vfrac{2}{IHeight} & 0 & -1 \\[\smallskipamount]
			0 & 0 & -\vfrac{2}{Far - Near} & -\vfrac{Far + Near}{Far - Near} \\[\smallskipamount]
			0 & 0 & 0 & 1
		\end{bmatrix}
	\end{equation}

	\begin{equation}\label{eq:projection-matrix}
		ExtendedOpenGLPerspectiveProjectionMatrix = glOrtho' \times Q'
	\end{equation}
}%


\subsection{Projector calibration}

High accuracy projection mapping requires proper hardware / software calibration of the camera / projector and also appropriate positioning within the intended workspace in order to avoid occlusions caused by the objects 3D shape or the human operators. The calibration process estimates the intrinsic parameters of the projector (that do not change when the projector is moved within the workspace) along with the extrinsic parameters that are needed to know where is the projector in the global reference frame in order to be able to do proper 3D rendering of the scene that will be projected.

The intrinsic parameters of a \gls{dlp} projector can be computed using image analysis of complementary gray code patterns (example in \cref{fig:dlp-calibration-pattern-wall}) projected into a chessboard. The calibration system proposed in \cite{Moreno2012} was used to retrieve the 5 intrinsic parameters (Fx, Fy, Cx, Cy, S) and 5 distortion coefficients of the projector along with the 3D position and rotation of the projector in relation to the camera (that remains firmly attached to the projector support for fast recalibration of the extrinsic parameters, as seen in \cref{fig:hardware}). It was used 5 sets of 42 gray code image patterns captured with the chessboard in different positions and orientations in relation to the projector, that was pointing to the table workspace at a distance of 0.81 meters.

For validating both the calibration and also the proposed camera modeling within \gls{opengl}, the 6 \gls{dof} pose of a chessboard (in relation to the camera) was estimated using the OpenCV calib3d module, followed by the computation of the 6 \gls{dof} pose of the projector in relation to the chessboard using the camera-projector extrinsic parameters. Then, a Gazebo virtual world was created with a virtual camera positioned in the 6 \gls{dof} pose computed earlier (for simulating the video projector) and a 3D model was placed at the origin of the virtual world with the dimensions matching the physical chessboard. After rendering an image using the intrinsic projector parameters and projecting it on top of the physical chessboard (shown in \cref{fig:dlp-projected-chessboard}), it can be seen that the white squares were projected into the chessboard with sub-millimeter accuracy.

\begin{figure}[H]
	\begin{floatrow}[2]
		\ffigbox[\FBwidth]
		{\includegraphics[height=.124\textheight]{dlp-calibration-pattern-wall}}
		{\caption{One of the projector gray code calibration patterns}\label{fig:dlp-calibration-pattern-wall}\vspace{-0.5em}}
		\ffigbox[\FBwidth]
		{\includegraphics[height=.124\textheight]{chessboard}}
		{\caption{Projector validation pattern (projected white squares)}\label{fig:dlp-projected-chessboard}\vspace{-0.5em}}
	\end{floatrow}
\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.47\linewidth]{chess-board-detection}
%	\caption{Camera pose estimation in relation to the chessboard origin (using the Kinect 2 RGB camera)}
%	\label{fig:chess-board-detection}
%\end{figure}


\subsection{Scene rendering}

For achieving accurate projection mapping, the Gazebo simulator\footnote{\url{http://gazebosim.org}} camera implementation was improved for allowing the setting of a custom projection matrix in order to perform 3D rendering with the camera model proposed earlier, that takes into account the projector intrinsic parameters. Moreover, it was added the possibility to dynamically change image, video and text during runtime for allowing the display of the relevant information for each assembly step.
For efficient 3D scene rendering, the Gazebo simulator relies on the cross platform open source Ogre3D graphics engine\footnote{\url{http://www.ogre3d.org}}, that in turn uses the \gls{opengl} \gls{gpu} \gls{api} to take advantage of the massively parallel graphics cards currently available to generate raster images for the \gls{dlp} projector (example of a rendered scene for the last assembly step in \cref{fig:scene-rendering}).

For user interaction the Gazebo simulator has a Qt\footnote{\url{https://www.qt.io}} \gls{gui} that allows visual inspection of the scene while also giving the option to add new objects or move and rotate existing models. Moreover, for lightweight rendering it can also start in server mode without a \gls{gui}.

\vspace{-0.5em}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{scene-rendering}
	\caption{3D scene rendering using the Gazebo simulator}
	\label{fig:scene-rendering}
	\vspace{-0.5em}
\end{figure}
\vspace{-0.5em}


\section{Human machine interface}\label{sec:human-machine-interaction}

The immersive \gls{hmi} developed (shown in \cref{fig:scene-rendering,fig:human-machine-interface}) projects into the workspace detailed textual information of the current assembly task along with a video showing the operation being performed by an expert operator. Given the high variability of assembly / maintenance operations, the system was designed to decompose the assembly process into a set of small and concise operations. This allows to keep the operator focused on the current task and reduces the required projection area. Moreover, the operator can pause and move the video forwards and backwards, allowing him to inspect a given complex operation with more time.

The user interaction with the projected \gls{hmi} is done by analyzing the 3D point cloud sensor data that falls within a set of \glspl{roi}, that are shown in \cref{fig:interaction-rois} as 4 green cubes for navigating within the assembly steps (first, previous, next and last), 1 dark blue box for pausing / playing the video and 1 yellow box for the video seek functionality (examples of a user interacting with the \gls{hmi} shown in \cref{fig:interaction-pause,fig:interaction-next,fig:interaction-seek}).

For ensuring robust detection of the user intentions, each \gls{roi} has a state machine that triggers its associated action only when a minimum number of points falls within the specified \gls{roi} boundaries (avoids sensor noise problems) and the user holds his finger within it for at least 0.25 seconds. Moreover, to avoid unintentionally triggering the same action several times, the user needs to remove and insert the finger into the \gls{roi} to request the action again.

When a \gls{roi} state machine activates its action, the 3D sensor data centroid (shown as spheres in \cref{fig:interaction-rois}) is computed for providing a visual debugging feedback of the \gls{hmi} state and also for being used in higher level perception, namely in the seek bar \gls{roi} (the vertical yellow box in \cref{fig:interaction-rois}), in which the seek time is computed by considering the relative position of the finger within the \gls{roi} (the bottom of the \gls{roi} is associated with the start of the current video while the top corresponds to the end of the current video).

\begin{figure}[H]
	\begin{floatrow}[2]
		\ffigbox[\FBwidth]
		{\includegraphics[height=.244\textheight]{human-machine-interface}}
		{\caption{Rendering of the human machine interface using Gazebo}\label{fig:human-machine-interface}\vspace{-0.5em}}
		\ffigbox[\FBwidth]
		{\includegraphics[height=.244\textheight]{interaction-rois}}
		{\caption{\glspl{roi} for the \gls{hmi} (overlaid on top of the Kinect 2 point cloud sensor data using Rviz)}\label{fig:interaction-rois}\vspace{-0.5em}}
	\end{floatrow}
\end{figure}

\vspace{-0.7em}
\begin{figure}[ht]
	\begin{floatrow}[3]
		\ffigbox[\FBwidth]
		{\includegraphics[height=.148\textheight]{interaction-pause}}
		{\caption{Example of video play / pause interaction}\label{fig:interaction-pause}\vspace{-0.5em}}
		\hspace{0.01em}
		\ffigbox[\FBwidth]
		{\includegraphics[height=.148\textheight]{interaction-next}}
		{\caption{Visual highlight of the request to move to the next assembly step}\label{fig:interaction-next}\vspace{-0.5em}}
		\hspace{0.01em}
		\ffigbox[\FBwidth]
		{\includegraphics[height=.148\textheight]{interaction-seek-2}}
		{\caption{Example of video seek interaction}\label{fig:interaction-seek}\vspace{-0.5em}}
	\end{floatrow}
\end{figure}
\vspace{-0.3em}


\section{Object 3D reconstruction}\label{sec:object-reconstruction}

For performing proper 3D rendering and also be able to estimate the 6 \gls{dof} pose of an object within the workspace, a 3D \gls{cad} or mesh model of the final product is required. Given the lack of public available \gls{cad} models for the Mitsubishi M000T20873 starter motor (shown in \cref{fig:starter-motor}), it was necessary to perform object 3D reconstruction. The 3D mesh model shown in \cref{fig:object-reconstruction} was generated using the David Laser 3D structured light system\footnote{\url{http://www.david-3d.com}}, and was built by surface matching algorithms using sensor data retrieved from 38 scans in which the starter motor was moved and rotated several times in order to capture enough sensor data for reconstructing the entire surface. This particular object created some challenges for the structured light scanner because it contains polished metallic sections and also black coated surfaces. As such, it was necessary to capture the same object sections several times with different projector brightness and camera exposure times (the dark regions required the maximum projector brightness and very high exposure times while the polished sections required dimmer projector brightness and very short exposure times for the surface to be fully reconstructed). Moreover, for ensuring that the point cloud matching algorithms worked as expected, all captured point clouds needed to be manually cleaned using the David Laser scanning software before performing the alignment and reconstruction of the mesh model.

The last step of the assembly process included a visual inspection stage performed by the operator in which he compared the final assembled product with a projected surface outline that was computed from the reconstructed mesh and then overlaid on top of the physical object after its 6 \gls{dof} pose was estimated. The outline was generated using a curvature estimator available in MeshLab\footnote{\url{http://www.meshlab.net}}. Namely, the "Compute curvature principal directions" filter was used with the principal component analysis method and the mean curvature color mapping algorithm (result shown in \cref{fig:object-reconstruction}). For improving the visibility of the outline, the color palette was remapped to show the high curvature regions (associated with surfaces boundaries) using the green color and the remaining intermediate curvatures were changed to very dark blue, for giving high contrast between the outline and the other surfaces. This was achieved by exporting the mesh to the .dae format (require by Gazebo for displaying colored models) with the color information separated to a .png texture. This way, it was possible to use Gimp\footnote{\url{https://www.gimp.org}} for remapping and enhancing the colors of the mesh outlines (as seen in \cref{fig:scene-rendering,fig:projection-mapping-2}).

\begin{figure}[!ht]
	\centering
	\includegraphics[height=.14\textheight]{mitsubishi-m000t20873-front}
	\hspace{1em}
	\includegraphics[height=.14\textheight]{mitsubishi-m000t20873-back}
	\caption{Mitsubishi M000T20873 starter motor}
	\label{fig:starter-motor}
	\vspace{-0.5em}
\end{figure}
\vspace{-0.5em}

\begin{figure}[!ht]
	\centering
	\includegraphics[height=.115\textheight]{object-reconstruction-front}
	\hspace{1em}
	\includegraphics[height=.115\textheight]{object-reconstruction-back}
	\caption{3D model of the starter motor reconstructed using the David Laser structured light 3D scanner}
	\label{fig:object-reconstruction}
	\vspace{-0.5em}
\end{figure}
\vspace{-0.5em}


\section{6 DoF pose estimation of objects}\label{sec:pose-estimation}

Robust and accurate estimation of the 6 \gls{dof} pose of objects is a requirement when virtual information must be overlaid on top of objects that may change their location over time. To achieve this goal, the 3D point cloud registration system (drl\footnote{\url{https://github.com/carlosmccosta/dynamic_robot_localization}}) described in \cite{Costa2016} was fine-tuned for our table top use case\footnote{\url{https://github.com/carlosmccosta/object_recognition}}. Namely, the reference point cloud preprocessing pipeline was configured to randomly select 3000 vertices of the starter motor reconstructed mesh (small green circles shown in \cref{fig:initial-pose-estimation}) and compute the \gls{sift} keypoints \cite{Lowe2004} (large yellow circles) and their associated \gls{fpfh} descriptors \cite{Rusu2009}. Later on, the registration pipeline for the sensor point clouds was setup. Its filtering stage was configured to segment the target object (starter motor) by extracting the points that were within a \gls{roi} specified in a calibrated frame on top of the table. The \gls{roi} defined the plausible space in which the object could be placed and also allowed to remove the points associated with the table. Moreover, for ensuring that the initial pose estimation would take less than 3 seconds, a random sample filter was also configured to downsample the point cloud within the \gls{roi} to 2500 points.

Given that in this use case only a single object appears within the \gls{roi}, the clustering algorithms were not added to the drl runtime pipeline. But they will be very useful in the future when extending the proposed \gls{sar} system to provide contextual information and visual feedback for all the parts visible by the sensor (the clustering stage will then provide input for a object recognition module, which will estimate which \gls{cad} is the best fit for each group of 3D points).

After finishing the setup of the preprocessing stage for the sensor point clouds pipeline, its initial alignment algorithms were configured and fine-tuned. Namely, the \gls{sift} keypoint detector along with the \gls{fpfh} keypoint descriptor and the \gls{ransac} feature matcher \cite{Rusu2009}. Moreover, the \gls{icp} \cite{Besl1992a} algorithm was also added for refining the point cloud registration in order to achieve a 6 \gls{dof} pose estimation with high accuracy (less than 2 mm of alignment error, as can be seen by the good overlap between the physical starter motor and its projected virtual outline displayed in \cref{fig:projection-mapping-2}).

The drl modules discussed earlier were enough for estimating the 6 \gls{dof} pose of objects, but for improving the efficiency of the drl, the tracking and recovery matching pipelines were also configured. These two pipelines allow the definition of a set of cloud matchers that are fine-tuned to align the sensor data by successively updating the 6 \gls{dof} pose of the object as new sensor data arrives. This allows to run the robust, but computationally intensive feature matching algorithms once, and then rely on dense point cloud matching algorithms to quickly and accurately update the object pose at the frame rate of the 3D sensor (up to 30 Hz, depending on the CPU used). The tracking pipeline was configured to rely on the \gls{icp} algorithm with the point-to-point metric with a search radius of 0.07 m for establishing correspondences during the iterative matching process and with the maximum number of iterations set to 300. On the other hand, the recovery pipeline was fine-tuned to use the \gls{icp} algorithm with the point-to-plane metric with the search radius and number of iterations increased to 0.2 m and 400 respectively. This approach of starting with robust feature matching algorithms and then rely on tracking algorithms with the sporadic usage of recovery methods was critical for being able to track the object with a reasonable frame rate, which will be useful in the future when extending the proposed \gls{sar} system to monitor the operator actions for ensuring that he is following the instructions and did not forget or misplaced assembly parts.

In \cref{fig:initial-pose-estimation} is shown an example of the estimation of the 6 \gls{dof} pose of the starter motor. The left image displays the overlay of the reference point cloud on top of the 3D sensor data in the previously estimated pose (before the operator occluded the part with its hand and moved it to a new place) while the right image shows the updated pose after alignment, that correctly detected that the operator moved the part to the right and rotated it 90º clockwise.

\vspace{-0.5em}
\begin{figure}[!ht]
	\centering
	\includegraphics[height=.11\textheight]{initial-pose-estimation-2-before}
	\includegraphics[height=.11\textheight]{initial-pose-estimation-2-after}
	\caption{Example of 6 DoF pose estimation of the assembled starter motor before (left) and after (right) the alignment of the 3D reconstructed mesh with the 3D sensor data}
	\label{fig:initial-pose-estimation}
	\vspace{-0.5em}
\end{figure}
\vspace{-1.5em}
